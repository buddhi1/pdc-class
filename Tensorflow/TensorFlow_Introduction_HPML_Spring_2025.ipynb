{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5u3a4csUPyn"
      },
      "source": [
        "#TensorFlow 2.0 Introduction\n",
        "In this notebook you will be given an interactive introduction to TensorFlow 2.0. We will walk through the following topics within the TensorFlow module:\n",
        "\n",
        "- TensorFlow Install and Setup\n",
        "- Representing Tensors\n",
        "- Tensor Shape and Rank\n",
        "- Types of Tensors\n",
        "\n",
        "\n",
        "If you'd like to follow along without installing TensorFlow on your machine you can use **Google Collaboratory**. Collaboratory is a free Jupyter notebook environment that requires no setup and runs entirely in the cloud."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7ThfbiQl96l"
      },
      "source": [
        "##Installing TensorFlow\n",
        "To install TensorFlow on your local machine you can use pip.\n",
        "```console\n",
        "pip install tensorflow\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieOzQ3TTuyzy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLH2EBldukw_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYQWyAJ2mez6"
      },
      "source": [
        "![alt text](https://)If you have a CUDA enabled GPU you can install the GPU version of TensorFlow. You will also need to install some other software which can be found here: https://www.tensorflow.org/install/gpu\n",
        "```console\n",
        "pip install tensorflow-gpu\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJjNMaSClWhg"
      },
      "source": [
        "## Importing TensorFlow\n",
        "The first step here is going to be to select the correct version of TensorFlow from within collabratory!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGcE8x2Gkw9K",
        "outputId": "cd573995-83eb-4121-a42f-1678db17b54d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4N7XbNDVY8P3",
        "outputId": "98d18c97-fe53-49ed-fa2e-66dc69341144"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.18.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf  # now import the tensorflow module\n",
        "print(tf.__version__)  # make sure the version is 2.x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duDj86TfWFof"
      },
      "source": [
        "##Tensors\n",
        "\"A tensor is a generalization of vectors and matrices to potentially higher dimensions. Internally, TensorFlow represents tensors as n-dimensional arrays of base datatypes.\" (https://www.tensorflow.org/guide/tensor)\n",
        "\n",
        "It should't surprise you that tensors are a fundemental apsect of TensorFlow. They are the main objects that are passed around and manipluated throughout the program. Each tensor represents a partialy defined computation that will eventually produce a value. TensorFlow programs work by building a graph of Tensor objects that details how tensors are related. Running different parts of the graph allow results to be generated.\n",
        "\n",
        "Each tensor has a data type and a shape.\n",
        "\n",
        "**Data Types Include**: float32, int32, string and others.\n",
        "\n",
        "**Shape**: Represents the dimension of data.\n",
        "\n",
        "Just like vectors and matrices tensors can have operations applied to them like addition, subtraction, dot product, cross product etc.\n",
        "\n",
        "In the next sections we will discuss some different properties of tensors. This is to make you more familiar with how tensorflow represnts data and how you can manipulate this data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAk6QhGUwQRt"
      },
      "source": [
        "###Creating Tensors\n",
        "Below is an example of how to create some different tensors.\n",
        "\n",
        "You simply define the value of the tensor and the datatype and you are good to go! It's worth mentioning that usually we deal with tensors of numeric data, it is quite rare to see string tensors.\n",
        "\n",
        "For a full list of datatypes please refer to the following guide.\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/dtypes/DType?version=stable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epGskXdjZHzu"
      },
      "outputs": [],
      "source": [
        "string = tf.Variable(\"this is a string\", tf.string)\n",
        "number = tf.Variable(324, tf.int16)\n",
        "floating = tf.Variable(3.567, tf.float64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV46tz-vwE5u"
      },
      "source": [
        "Difference between tf.Tensor and tf.*Variable*\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78kX2FLbwA3X",
        "outputId": "f3f968c9-03a5-427b-e933-66d263c5e8dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Creating a Tensor (Immutable)\n",
        "tensor = tf.constant(5.0)\n",
        "#tensor.assign(10.0)  # ❌ This will raise an error\n",
        "# Creating a Variable (Mutable)\n",
        "variable = tf.Variable(5.0)\n",
        "variable.assign(10.0)  # ✅ Allowed\n",
        "print(variable.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOBRMKFF03P8"
      },
      "source": [
        "Tf.Variable as Tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYfhxemr01b6",
        "outputId": "78af8a9d-7414-4d5a-e38f-66be9d027a12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6.0\n"
          ]
        }
      ],
      "source": [
        "a = tf.Variable(3.0)\n",
        "b = tf.constant(2.0)\n",
        "c = a * b  # Works like a tensor operation\n",
        "print(c.numpy())  # Output: 6.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0_H71HMaE-5"
      },
      "source": [
        "###Rank/Degree of Tensors\n",
        "Another word for rank is degree, these terms simply mean the number of dimensions involved in the tensor. What we created above is a *tensor of rank 0*, also known as a scalar.\n",
        "\n",
        "Now we'll create some tensors of higher degrees/ranks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hX_Cc5IfjQ6-"
      },
      "outputs": [],
      "source": [
        "rank1_tensor = tf.Variable( [\"Test\"], tf.string )\n",
        "rank2_tensor = tf.Variable( [[\"test\", \"ok\"], [\"test\", \"yes\"]], tf.string )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZXYhxd11ddU",
        "outputId": "9c4c4449-24b7-4936-f674-0c4cbbf99c1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1,)\n",
            "(2, 2)\n"
          ]
        }
      ],
      "source": [
        "print(rank1_tensor.shape)\n",
        "print(rank2_tensor.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55zuGMc7nHjC"
      },
      "source": [
        "**To determine the rank** of a tensor we can call the following method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zrj0rAWLnMNv",
        "outputId": "482625aa-bd6e-4f70-89b8-9cb853abf61b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=int32, numpy=2>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# If a model throws shape mismatch errors, tf.rank can help identify incorrect tensor shapes.\n",
        "tf.rank(rank2_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTv4Gz67pQbx"
      },
      "source": [
        "The rank of a tensor is direclty related to the deepest level of nested lists. You can see in the first example ```[\"Test\"]``` is a rank 1 tensor as the deepest level of nesting is 1.\n",
        "Where in the second example ```[[\"test\", \"ok\"], [\"test\", \"yes\"]]``` is a rank 2 tensor as the deepest level of nesting is 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaVrANK8q21q"
      },
      "source": [
        "###Shape of Tensors\n",
        "Now that we've talked about the rank of tensors it's time to talk about the shape. The shape of a tensor is simply the number of elements that exist in each dimension. TensorFlow will try to determine the shape of a tensor but sometimes it may be unknown.\n",
        "\n",
        "To **get the shape** of a tensor we use the shape attribute.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_NRXsFOraYa",
        "outputId": "8afa8d04-dabc-4704-c1f7-87d68fda014e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "rank2_tensor.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVDmLJeFs086"
      },
      "source": [
        "###Changing Shape\n",
        "The number of elements of a tensor is the product of the sizes of all its shapes. There are often many shapes that have the same number of elements, making it convient to be able to change the shape of a tensor.\n",
        "\n",
        "The example below shows how to change the shape of a tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZ8Rbs2xtNqj"
      },
      "outputs": [],
      "source": [
        "tensor0 = tf.zeros([5])\n",
        "tensor1 = tf.ones([1,2,3])  # tf.ones() creates a shape [1,2,3] tensor full of ones\n",
        "tensor2 = tf.reshape(tensor1, [2,3,1])  # reshape existing data to shape [2,3,1]\n",
        "tensor3 = tf.reshape(tensor2, [3, -1])  # -1 tells the tensor to calculate the size of the dimension in that place\n",
        "                                        # this will reshape the tensor to [3,3]\n",
        "\n",
        "# The numer of elements in the reshaped tensor MUST match the number in the original"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M631k7UDv1Wh"
      },
      "source": [
        "Now let's have a look at our different tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFNmUxaEv6s3",
        "outputId": "41b1771a-aaaa-4087-a024-9e696f731c18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0. 0. 0. 0. 0.], shape=(5,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[1. 1. 1.]\n",
            "  [1. 1. 1.]]], shape=(1, 2, 3), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[[1.]\n",
            "  [1.]\n",
            "  [1.]]\n",
            "\n",
            " [[1.]\n",
            "  [1.]\n",
            "  [1.]]], shape=(2, 3, 1), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[1. 1.]\n",
            " [1. 1.]\n",
            " [1. 1.]], shape=(3, 2), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "print(tensor0)\n",
        "print(tensor1)\n",
        "print(tensor2)\n",
        "print(tensor3)\n",
        "# Notice the changes in shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q88pJucBolsp"
      },
      "source": [
        "###Slicing Tensors\n",
        "You may be familiar with the term \"slice\" in python and its use on lists, tuples etc. Well the slice operator can be used on tensors to select specific axes or elements.\n",
        "\n",
        "When we slice or select elements from a tensor, we can use comma seperated values inside the set of square brackets. Each subsequent value refrences a different dimension of the tensor.\n",
        "\n",
        "Ex: ```tensor[dim1, dim2, dim3]```\n",
        "\n",
        "I've included a few examples that will hopefully help illustrate how we can manipulate tensors with the slice operator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0YrD-hRqD-W",
        "outputId": "e121989b-e190-4006-9505-1cc4b6cf5bae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(2, shape=(), dtype=int32)\n",
            "(4, 5)\n"
          ]
        }
      ],
      "source": [
        "# Creating a 2D tensor\n",
        "matrix = [[1,2,3,4,5],\n",
        "          [6,7,8,9,10],\n",
        "          [11,12,13,14,15],\n",
        "          [16,17,18,19,20]]\n",
        "\n",
        "tensor = tf.Variable(matrix, dtype=tf.int32)\n",
        "print(tf.rank(tensor))\n",
        "print(tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd85uGI7qyfC",
        "outputId": "b8501fea-d0ae-4725-f675-2c5cbcb0817e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(3, shape=(), dtype=int32)\n",
            "tf.Tensor([1 2 3 4 5], shape=(5,), dtype=int32)\n",
            "tf.Tensor([ 1  6 11 16], shape=(4,), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[ 6  7  8  9 10]\n",
            " [16 17 18 19 20]], shape=(2, 5), dtype=int32)\n",
            "tf.Tensor([ 6 11], shape=(2,), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "# Now lets select some different rows and columns from our tensor\n",
        "\n",
        "three = tensor[0,2]  # selects the 3rd element from the 1st row\n",
        "print(three)  # -> 3\n",
        "row1 = tensor[0]  # selects the first row\n",
        "print(row1)\n",
        "column1 = tensor[:, 0]  # selects the first column\n",
        "print(column1)\n",
        "row_2_and_4 = tensor[1::2]  # selects second and fourth row\n",
        "print(row_2_and_4)\n",
        "column_1_in_row_2_and_3 = tensor[1:3, 0]\n",
        "print(column_1_in_row_2_and_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6w6FVBMk5II5"
      },
      "source": [
        "#Introduction to Neural Networks\n",
        "In this notebook you will learn how to create and use a neural network to classify articles of clothing. To achieve this, we will use a sub module of TensorFlow called *keras*.\n",
        "\n",
        "*This guide is based on the following TensorFlow documentation.*\n",
        "\n",
        "https://www.tensorflow.org/tutorials/keras/classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFQqW9r-ikJb"
      },
      "source": [
        "##Keras\n",
        "Before we dive in and start discussing neural networks, I'd like to give a breif introduction to keras.\n",
        "\n",
        "From the keras official documentation (https://keras.io/) keras is described as follows.\n",
        "\n",
        "\"Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation.\n",
        "\n",
        "Use Keras if you need a deep learning library that:\n",
        "\n",
        "- Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).\n",
        "- Supports both convolutional networks and recurrent networks, as well as combinations of the two.\n",
        "- Runs seamlessly on CPU and GPU.\"\n",
        "\n",
        "Keras is a very powerful module that allows us to avoid having to build neural networks from scratch. It also hides a lot of mathematical complexity (that otherwise we would have to implement) inside of helpful packages, modules and methods.\n",
        "\n",
        "In this guide we will use keras to quickly develop neural networks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hivk879ZQhxU"
      },
      "source": [
        "##What is a Neural Network\n",
        "So, what are these magical things that have been beating chess grandmasters, driving cars, detecting cancer cells and winning video games?\n",
        "\n",
        "A deep neural network is a layered representation of data. The term \"deep\" refers to the presence of multiple layers. Recall that in our core learning algorithms (like linear regression) data was not transformed or modified within the model, it simply existed in one layer. We passed some features to our model, some math was done, an answer was returned. The data was not changed or transformed throughout this process. A neural network processes our data differently. It attempts to represent our data in different ways and in different dimensions by applying specific operations to transform our data at each layer. Another way to express this is that at each layer our data is transformed in order to learn more about it. By performing these transformations, the model can better understand our data and therefore provide a better prediction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOqUCZ2klTAq"
      },
      "source": [
        "##How it Works\n",
        "Before going into too much detail I will provide a very surface level explination of how neural networks work on a mathematical level. All the terms and concepts I discuss will be defined and explained in more detail below.\n",
        "\n",
        "On a lower level neural networks are simply a combination of elementry math operations and some more advanced linear algebra. Each neural network consists of a sequence of layers in which data passes through. These layers are made up on neurons and the neurons of one layer are connected to the next (see below). These connections are defined by what we call a weight (some numeric value). Each layer also has something called a bias, this is simply an extra neuron that has no connections and holds a single numeric value. Data starts at the input layer and is trasnformed as it passes through subsequent layers. The data at each subsequent neuron is defined as the following.\n",
        "\n",
        "> $Y =(\\sum_{i=0}^n w_i x_i) + b$\n",
        "\n",
        "> $w$ stands for the weight of each connection to the neuron\n",
        "\n",
        "> $x$ stands for the value of the connected neuron from the previous value\n",
        "\n",
        "> $b$ stands for the bias at each layer, this is a constant\n",
        "\n",
        "> $n$ is the number of connections\n",
        "\n",
        "> $Y$ is the output of the current neuron\n",
        "\n",
        "> $\\sum$ stands for sum\n",
        "\n",
        "The equation you just read is called a weighed sum. We will take this weighted sum at each and every neuron as we pass information through the network. Then we will add what's called a bias to this sum. The bias allows us to shift the network up or down by a constant value. It is like the y-intercept of a line.\n",
        "\n",
        "But that equation is the not complete one! We forgot a crucial part, **the activation function**. This is a function that we apply to the equation seen above to add complexity and dimensionality to our network. Our new equation with the addition of an activation function $F(x)$ is seen below.\n",
        "\n",
        "> $Y =F((\\sum_{i=0}^n w_i x_i) + b)$\n",
        "\n",
        "Our network will start with predefined activation functions (they may be different at each layer) but random weights and biases. As we train the network by feeding it data it will learn the correct weights and biases and adjust the network accordingly using a technqiue called **backpropagation** (explained below). Once the correct weights and biases have been learned our network will hopefully be able to give us meaningful predictions. We get these predictions by observing the values at our final layer, the output layer.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3rlxsik57ji"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-oMh18_j5kl"
      },
      "source": [
        "##Breaking Down The Neural Network!\n",
        "\n",
        "Before we dive into any code lets break down how a neural network works and what it does.\n",
        "\n",
        "![alt text](http://www.extremetech.com/wp-content/uploads/2015/07/NeuralNetwork.png)\n",
        "*Figure 1*\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9hd-R1ulSdp"
      },
      "source": [
        "###Data\n",
        "The type of data a neural network processes varies drastically based on the problem being solved. When we build a neural network, we define what shape and kind of data it can accept. It may sometimes be neccessary to modify our dataset so that it can be passed to our neural network.\n",
        "\n",
        "Some common types of data a neural network uses are listed below.\n",
        "- Vector Data (2D)\n",
        "- Timeseries or Sequence (3D)\n",
        "- Image Data (4D)\n",
        "- Video Data (5D)\n",
        "\n",
        "There are of course many different types or data, but these are the main categories.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xyxxs7oMlWtz"
      },
      "source": [
        "###Layers\n",
        "As we mentioned earlier each neural network consists of multiple layers. At each layer a different transformation of data occurs. Our initial input data is fed through the layers and eventually arrives at the output layer where we will obtain the result.\n",
        "####Input Layer\n",
        "The input layer is the layer that our initial data is passed to. It is the first layer in our neural network.\n",
        "####Output Layer\n",
        "The output layer is the layer that we will retrive our results from. Once the data has passed through all other layers it will arrive here.\n",
        "####Hidden Layer(s)\n",
        "All the other layers in our neural network are called \"hidden layers\". This is because they are hidden to us, we cannot observe them. Most neural networks consist of at least one hidden layer but can have an unlimited amount. Typically, the more complex the model the more hidden layers.\n",
        "####Neurons\n",
        "Each layer is made up of what are called neurons. Neurons have a few different properties that we will discuss later. The important aspect to understand now is that each neuron is responsible for generating/holding/passing ONE numeric value.\n",
        "\n",
        "This means that in the case of our input layer it will have as many neurons as we have input information. For example, say we want to pass an image that is 28x28 pixels, thats 784 pixels. We would need 784 neurons in our input layer to capture each of these pixels.\n",
        "\n",
        "This also means that our output layer will have as many neurons as we have output information. The output is a little more complicated to understand so I'll refrain from an example right now but hopefully you're getting the idea.\n",
        "\n",
        "But what about our hidden layers? Well these have as many neurons as we decide. We'll discuss how we can pick these values later but understand a hidden layer can have any number of neurons.\n",
        "####Connected Layers\n",
        "So how are all these layers connected? Well the neurons in one layer will be connected to neurons in the subsequent layer. However, the neurons can be connected in a variety of different ways.\n",
        "\n",
        "Take for example *Figure 1* (look above). Each neuron in one layer is connected to every neuron in the next layer. This is called a **dense** layer. There are many other ways of connecting layers but well discuss those as we see them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_bM6nQ-PZBY"
      },
      "source": [
        "###Weights\n",
        "Weights are associated with each connection in our neural network. Every pair of connected nodes will have one weight that denotes the strength of the connection between them. These are vital to the inner workings of a neural network and will be tweaked as the neural network is trained. The model will try to determine what these weights should be to achieve the best result. Weights start out at a constant or random value and will change as the network sees training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwYq9doXeIl-"
      },
      "source": [
        "###Biases\n",
        "Biases are another important part of neural networks and will also be tweaked as the model is trained. A bias is simply a constant value associated with each layer. It can be thought of as an extra neuron that has no connections. The purpose of a bias is to shift an entire activation function by a constant value. This allows a lot more flexibllity when it comes to choosing an activation and training the network. There is one bias for each layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F92rhvd6PcRI"
      },
      "source": [
        "###Activation Function\n",
        "Activation functions are simply a function that is applied to the weighed sum of a neuron. They can be anything we want but are typically higher order/degree functions that aim to add a higher dimension to our data. We would want to do this to introduce more comolexity to our model. By transforming our data to a higher dimension, we can typically make better, more complex predictions.\n",
        "\n",
        "A list of some common activation functions and their graphs can be seen below.\n",
        "\n",
        "- Relu (Rectified Linear Unit)\n",
        "\n",
        "![alt text](https://yashuseth.files.wordpress.com/2018/02/relu-function.png?w=309&h=274)\n",
        "- Tanh (Hyperbolic Tangent)\n",
        "\n",
        "![alt text](http://mathworld.wolfram.com/images/interactive/TanhReal.gif)\n",
        "- Sigmoid\n",
        "\n",
        "![alt text](https://miro.medium.com/max/970/1*Xu7B5y9gp0iL5ooBj7LtWw.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2xNjpctlBUM"
      },
      "source": [
        "###Backpropagation\n",
        "Backpropagation is the fundemental algorithm behind training neural networks. It is what changes the weights and biases of our network. To fully explain this process, we need to start by discussing something called a cost/loss function.\n",
        "\n",
        "####Loss/Cost Function\n",
        "As we now know our neural network feeds information through the layers until it eventually reaches an output layer. This layer contains the results that we look at to determine the prediciton from our network. In the training phase it is likely that our network will make many mistakes and poor predicitions. In fact, at the start of training our network doesn't know anything (it has random weights and biases)!\n",
        "\n",
        "We need some way of evaluating if the network is doing well and how well it is doing. For our training data we have the features (input) and the labels (expected output), because of this we can compare the output from our network to the expected output. Based on the difference between these values we can determine if our network has done a good job or poor job. If the network has done a good job, we'll make minor changes to the weights and biases. If it has done a poor job our changes may be more drastic.\n",
        "\n",
        "So, this is where the cost/loss function comes in. This function is responsible for determining how well the network did. We pass it the output and the expected output, and it returns to us some value representing the cost/loss of the network. This effectively makes the networks job to optimize this cost function, trying to make it as low as possible.\n",
        "\n",
        "Some common loss/cost functions include.\n",
        "- Mean Squared Error\n",
        "- Mean Absolute Error\n",
        "- Hinge Loss\n",
        "\n",
        "####Gradient Descent\n",
        "Gradient descent and backpropagation are closely related. Gradient descent is the algorithm used to find the optimal paramaters (weights and biases) for our network, while backpropagation is the process of calculating the gradient that is used in the gradient descent step.\n",
        "\n",
        "Gradient descent requires some pretty advanced calculus and linear algebra to understand so we'll stay away from that for now. Let's just read the formal definition for now.\n",
        "\n",
        "\"Gradient descent is an optimization algorithm used to minimize some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient. In machine learning, we use gradient descent to update the parameters of our model.\" (https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html)\n",
        "\n",
        "And that's all we really need to know for now. I'll direct you to the video for a more in depth explination.\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1000/1*iU1QCnSTKrDjIPjSAENLuQ.png)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KiTMDCKlBI7"
      },
      "source": [
        "###Optimizer\n",
        "You may sometimes see the term optimizer or optimization function. This is simply the function that implements the backpropagation algorithm described above. Here's a list of a few common ones.\n",
        "- Gradient Descent\n",
        "- Stochastic Gradient Descent\n",
        "- Mini-Batch Gradient Descent\n",
        "- Momentum\n",
        "- Nesterov Accelerated Gradient\n",
        "\n",
        "*This article explains them quite well is where I've pulled this list from.*\n",
        "\n",
        "(https://medium.com/@sdoshi579/optimizers-for-training-neural-network-59450d71caf6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kc5hFCLSiDNr"
      },
      "source": [
        "##Creating a Neural Network\n",
        "Okay now you have reached the exciting part of this tutorial! No more math and complex explinations. Time to get hands on and train a very basic neural network.\n",
        "\n",
        "*As stated earlier this guide is based off of the following TensorFlow tutorial.*\n",
        "https://www.tensorflow.org/tutorials/keras/classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HI_aWoNd57m7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3io6gbUrjOQY"
      },
      "source": [
        "###Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJY9sfWn57qG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8t_EdO8jEHz",
        "outputId": "c3e9a968-95b0-4023-e26b-e5a270813313"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZD-7otz57ss"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_iFN10li6V1"
      },
      "source": [
        "###Dataset\n",
        "For this tutorial we will use the MNIST Fashion Dataset. This is a dataset that is included in keras.\n",
        "\n",
        "This dataset includes 60,000 images for training and 10,000 images for validation/testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQmVmgOxjCOV",
        "outputId": "5235f997-1ac2-4242-f16a-594286920a20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist  # load dataset\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()  # split into tetsing and training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcIall2njfn1"
      },
      "source": [
        "Let's have a look at this data to see what we are working with.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhLXRxOdjisI",
        "outputId": "744870af-55e9-4c97-fe90-4fd4937effe4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "train_images.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYyu4PYx7mKf"
      },
      "source": [
        "So we've got 60,000 images that are made up of 28x28 pixels (784 in total)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m280zyPqj3ws",
        "outputId": "1634f61d-ab22-4398-ec66-4d128d32f35d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "194"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "train_images[0,23,23]  # let's have a look at one pixel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTeCXaAt7uW2"
      },
      "source": [
        "Our pixel values are between 0 and 255, 0 being black and 255 being white. This means we have a grayscale image as there are no color channels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rn78KO7fkQPJ",
        "outputId": "2d4f92cb-a205-4678-888d-14bc1829c828"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9, 0, 0, 3, 0, 2, 7, 2, 5, 5], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "train_labels[:10]  # let's have a look at the first 10 training labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r90qZKsnkaW7"
      },
      "source": [
        "Our labels are integers ranging from 0 - 9. Each integer represents a specific article of clothing. We'll create an array of label names to indicate which is which."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBiICD2tkne8"
      },
      "outputs": [],
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rv06eD8krMR"
      },
      "source": [
        "Fianlly let's look at what some of these images look like!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_OAp7I77UVX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "Nfc8LV4Pkq0X",
        "outputId": "dd164deb-e603-4daa-d0de-81cc29f909c4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure()\n",
        "plt.imshow(train_images[1])\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_DC1b0grL1N"
      },
      "source": [
        "##Data Preprocessing\n",
        "The last step before creating our model is to *preprocess* our data. This simply means applying some prior transformations to our data before feeding it the model. In this case we will simply scale all our greyscale pixel values (0-255) to be between 0 and 1. We can do this by dividing each value in the training and testing sets by 255.0. We do this because smaller values will make it easier for the model to process our values.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUVabJ197UdX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHde8MYW0OQo"
      },
      "outputs": [],
      "source": [
        "train_images = train_images / 255.0\n",
        "\n",
        "test_images = test_images / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHOX6GqR0QuD"
      },
      "source": [
        "##Building the Model\n",
        "Now it's time to build the model! We are going to use a keras *sequential* model with three different layers. This model represents a feed-forward neural network (one that passes values from left to right). We'll break down each layer and its architecture below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2eXMyjP7Ugl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDxodHMv0xgG",
        "outputId": "8dae8d0b-094f-4e3e-866f-3034caf8de0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ],
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),  # input layer (1)\n",
        "    keras.layers.Dense(128, activation='relu'),  # hidden layer (2)\n",
        "    keras.layers.Dense(10, activation='softmax') # output layer (3)\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-bL-I5w0414"
      },
      "source": [
        "**Layer 1:** This is our input layer and it will conist of 784 neurons. We use the flatten layer with an input shape of (28,28) to denote that our input should come in in that shape. The flatten means that our layer will reshape the shape (28,28) array into a vector of 784 neurons so that each pixel will be associated with one neuron.\n",
        "\n",
        "**Layer 2:** This is our first and only hidden layer. The *dense* denotes that this layer will be fully connected and each neuron from the previous layer connects to each neuron of this layer. It has 128 neurons and uses the rectify linear unit activation function.\n",
        "\n",
        "**Layer 3:** This is our output later and is also a dense layer. It has 10 neurons that we will look at to determine our models output. Each neuron represnts the probabillity of a given image being one of the 10 different classes. The activation function *softmax* is used on this layer to calculate a probabillity distribution for each class. This means the value of any neuron in this layer will be between 0 and 1, where 1 represents a high probabillity of the image being that class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-j1UF9QH21Ex"
      },
      "source": [
        "###Compile the Model\n",
        "The last step in building the model is to define the loss function, optimizer and metrics we would like to track. I won't go into detail about why we chose each of these right now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Msigq4Ja29QX"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YYW5V_53OXV"
      },
      "source": [
        "##Training the Model\n",
        "Now it's finally time to train the model. Since we've already done all the work on our data this step is as easy as calling a single method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmAtc4uI3_C7",
        "outputId": "a681baa9-0231-46c5-8167-f8400581af15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7812 - loss: 0.6331\n",
            "Epoch 2/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8600 - loss: 0.3866\n",
            "Epoch 3/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8775 - loss: 0.3346\n",
            "Epoch 4/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.8841 - loss: 0.3150\n",
            "Epoch 5/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8913 - loss: 0.2957\n",
            "Epoch 6/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8986 - loss: 0.2761\n",
            "Epoch 7/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.9015 - loss: 0.2643\n",
            "Epoch 8/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9068 - loss: 0.2522\n",
            "Epoch 9/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9069 - loss: 0.2449\n",
            "Epoch 10/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9158 - loss: 0.2271\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x78bad4184810>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "model.fit(train_images, train_labels, epochs=10)  # we pass the data, labels and epochs and watch the magic!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6SRtNcF4K1O"
      },
      "source": [
        "##Evaluating the Model\n",
        "Now it's time to test/evaluate the model. We can do this quite easily using another builtin method from keras.\n",
        "\n",
        "The *verbose* argument is defined from the keras documentation as:\n",
        "\"verbose: 0 or 1. Verbosity mode. 0 = silent, 1 = progress bar.\"\n",
        "(https://keras.io/models/sequential/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqI0FEO54XN1",
        "outputId": "05b15808-27f6-4cc5-c699-d931e4d40743"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8757 - loss: 0.3523\n",
            "Test accuracy: 0.8737000226974487\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=1)\n",
        "\n",
        "print('Test accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb4_EtfK5DuW"
      },
      "source": [
        "You'll likely notice that the accuracy here is lower than when training the model. This difference is reffered to as **overfitting**.\n",
        "\n",
        "And now we have a trained model that's ready to use to predict some values!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pv0XpgwJ7GlW"
      },
      "source": [
        "##Making Predictions\n",
        "To make predictions we simply need to pass an array of data in the form we've specified in the input layer to ```.predict()``` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMAkNWii7Ufj",
        "outputId": "7151b8f8-bc87-4a6c-e50e-f9644f70a884"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n"
          ]
        }
      ],
      "source": [
        "predictions = model.predict(test_images)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EP6-8YCAh_kI",
        "outputId": "3bf08b97-4dfb-4073-9130-951cda7f5862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5.4976050e-09, 1.3917956e-12, 5.8071848e-08, ..., 2.0412288e-03,\n",
              "        2.6799578e-09, 9.8228884e-01],\n",
              "       [4.6612808e-05, 2.7446659e-13, 9.8005193e-01, ..., 1.2781725e-19,\n",
              "        2.2247501e-10, 7.7286979e-15],\n",
              "       [4.4362399e-11, 9.9999994e-01, 4.8132308e-16, ..., 3.4320561e-27,\n",
              "        3.1154554e-15, 5.9534847e-24],\n",
              "       ...,\n",
              "       [3.5526671e-03, 6.2208692e-09, 1.2607413e-04, ..., 7.1735453e-09,\n",
              "        9.9517363e-01, 1.8125623e-10],\n",
              "       [1.5334402e-09, 9.9999183e-01, 8.9711008e-13, ..., 6.4269445e-16,\n",
              "        1.0870734e-09, 1.0886282e-15],\n",
              "       [4.4461776e-06, 1.1125788e-09, 8.6029452e-08, ..., 9.8157930e-04,\n",
              "        1.0685104e-04, 5.0351723e-06]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiRNg9Yr7lCt"
      },
      "source": [
        "If we wan't to get the value with the highest score we can use a useful function from numpy called ```argmax()```. This simply returns the index of the maximium value from a numpy array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaagMfi671ci",
        "outputId": "1f42e44d-7918-4a1c-d4ce-0e3388e6ed24"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "np.argmax(predictions[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWY4SKYm8h93"
      },
      "source": [
        "And we can check if this is correct by looking at the value of the cooresponding test label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVNepduo8nEy",
        "outputId": "e3ef66cd-7272-4ec6-a9b2-b0fe333969de"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "test_labels[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8I1EqJu8qRl"
      },
      "source": [
        "##Verifying Predictions\n",
        "I've written a small function here to help us verify predictions with some simple visuals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "-HJV4JF789aC",
        "outputId": "91b897a1-dd11-4cc2-9cca-d009f81aba89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pick a number: 156\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "COLOR = 'white'\n",
        "plt.rcParams['text.color'] = COLOR\n",
        "plt.rcParams['axes.labelcolor'] = COLOR\n",
        "def predict(model, image, correct_label):\n",
        "  class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "  prediction = model.predict(np.array([image]))\n",
        "  predicted_class = class_names[np.argmax(prediction)]\n",
        "  show_image(image, class_names[correct_label], predicted_class)\n",
        "\n",
        "def show_image(img, label, guess):\n",
        "  plt.figure()\n",
        "  plt.imshow(img, cmap=plt.cm.binary)\n",
        "  plt.title(\"Excpected: \" + label)\n",
        "  plt.xlabel(\"Guess: \" + guess)\n",
        "  plt.colorbar()\n",
        "  plt.grid(False)\n",
        "  plt.show()\n",
        "\n",
        "def get_number():\n",
        "  while True:\n",
        "    num = input(\"Pick a number: \")\n",
        "    if num.isdigit():\n",
        "      num = int(num)\n",
        "      if 0 <= num <= 1000:\n",
        "        return int(num)\n",
        "    else:\n",
        "      print(\"Try again...\")\n",
        "\n",
        "num = get_number()\n",
        "image = test_images[num]\n",
        "label = test_labels[num]\n",
        "predict(model, image, label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1TlT7aG2_jyW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2OoXbe7aSVl"
      },
      "source": [
        "#Sources\n",
        "Most of the information is taken direclty from the TensorFlow website which can be found below.\n",
        "\n",
        "https://www.tensorflow.org/guide/tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_o2L3Io9t4c"
      },
      "source": [
        "#Deep Computer Vision\n",
        "\n",
        "In this guide we will learn how to peform *image classification and object detection/recognition* using deep computer vision with something called a **convolutional neural network**.\n",
        "\n",
        "The goal of our convolutional neural networks will be to classify and detect images or specific objects from within the image. We will be using image data as our features and a label for those images as our label or output.\n",
        "\n",
        "We already know how neural networks work so we can skip through the basics and move right into explaining the following concepts.\n",
        "- Image Data\n",
        "- Convolutional Layer\n",
        "- Pooling Layer\n",
        "- CNN Architectures\n",
        "\n",
        "The major differences we are about to see in these types of neural networks are the layers that make them up."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdqlqfhLCHZl"
      },
      "source": [
        "##Image Data\n",
        "So far, we have dealt with pretty straight forward data that has 1 or 2 dimensions. Now we are about to deal with image data that is usually made up of 3 dimensions. These 3 dimensions are as follows:\n",
        "- image height\n",
        "- image width\n",
        "- color channels\n",
        "\n",
        "The only item in the list above you may not understand is **color channels**. The number of color channels represents the depth of an image and coorelates to the colors used in it. For example, an image with three channels is likely made up of rgb (red, green, blue) pixels. So, for each pixel we have three numeric values in the range 0-255 that define its color. For an image of color depth 1 we would likely have a greyscale image with one value defining each pixel, again in the range of 0-255.\n",
        "\n",
        "![alt text](http://xrds.acm.org/blog/wp-content/uploads/2016/06/Figure1.png)\n",
        "\n",
        "Keep this in mind as we discuss how our network works and the input/output of each layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mqznmTh--v2"
      },
      "source": [
        "##Convolutional Neural Network\n",
        "**Note:** I will use the term *convnet* and convolutional neural network interchangably.\n",
        "\n",
        "Each convolutional neural network is made up of one or many convolutional layers. These layers are different than the *dense* layers we have seen previously. Their goal is to find patterns from within images that can be used to classify the image or parts of it. But this may sound familiar to what our densly connected neural network in the previous section was doing, well that's becasue it is.\n",
        "\n",
        "The fundemental difference between a dense layer and a convolutional layer is that dense layers detect patterns globally while convolutional layers detect patterns locally. When we have a densly connected layer each node in that layer sees all the data from the previous layer. This means that this layer is looking at all the information and is only capable of analyzing the data in a global capacity. Our convolutional layer however will not be densly connected, this means it can detect local patterns using part of the input data to that layer.\n",
        "\n",
        "*Let's have a look at how a densly connected layer would look at an image vs how a convolutional layer would.*\n",
        "\n",
        "This is our image; the goal of our network will be to determine whether this image is a cat or not.\n",
        "![alt text](https://img.webmd.com/dtmcms/live/webmd/consumer_assets/site_images/article_thumbnails/reference_guide/cat_weight_ref_guide/1800x1200_cat_weight_ref_guide.jpg)\n",
        "\n",
        "**Dense Layer:** A dense layer will consider the ENTIRE image. It will look at all the pixels and use that information to generate some output.\n",
        "\n",
        "**Convolutional Layer:** The convolutional layer will look at specific parts of the image. In this example let's say it analyzes the highlighted parts below and detects patterns there.\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1M7v7S-b-zisFLI_G4ZY_RdUJQrGpJ3zt)\n",
        "\n",
        "Can you see why this might make these networks more useful?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIQvxFu_FB3h"
      },
      "source": [
        "###How They Work\n",
        "A dense neural network learns patterns that are present in one specific area of an image. This means if a pattern that the network knows is present in a different area of the image it will have to learn the pattern again in that new area to be able to detect it.\n",
        "\n",
        "*Let's use an example to better illustrate this.*\n",
        "\n",
        "We'll consider that we have a dense neural network that has learned what an eye looks like from a sample of dog images.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=16FJKkVS_lZToQOCOOy6ohUpspWgtoQ-c)\n",
        "\n",
        "Let's say it's determined that an image is likely to be a dog if an eye is present in the boxed off locations of the image above.\n",
        "\n",
        "Now let's flip the image.\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1V7Dh7BiaOvMq5Pm_jzpQfJTZcpPNmN0W)\n",
        "\n",
        "Since our densly connected network has only recognized patterns globally it will look where it thinks the eyes should be present. Clearly it does not find them there and therefore would likely determine this image is not a dog. Even though the pattern of the eyes is present, it's just in a different location.\n",
        "\n",
        "Since convolutional layers learn and detect patterns from different areas of the image, they don't have problems with the example we just illustrated. They know what an eye looks like and by analyzing different parts of the image can find where it is present.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20J29gz-NroA"
      },
      "source": [
        "###Multiple Convolutional Layers\n",
        "In our models it is quite common to have more than one convolutional layer. Even the basic example we will use in this guide will be made up of 3 convolutional layers. These layers work together by increasing complexity and abstraction at each subsequent layer. The first layer might be responsible for picking up edges and short lines, while the second layer will take as input these lines and start forming shapes or polygons. Finally, the last layer might take these shapes and determine which combiantions make up a specific image.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ii-a9rXzRwNi"
      },
      "source": [
        "##Feature Maps\n",
        "You may see me use the term *feature map* throughout this tutorial. This term simply stands for a 3D tensor with two spacial axes (width and height) and one depth axis. Our convolutional layers take feature maps as their input and return a new feature map that reprsents the prescence of spcific filters from the previous feature map. These are what we call *response maps*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OScABB-ScXHx"
      },
      "source": [
        "##Layer Parameters\n",
        "A convolutional layer is defined by two key parameters.\n",
        "\n",
        "####**Filters**\n",
        "A filter is a m x n pattern of pixels that we are looking for in an image. The number of filters in a convolutional layer reprsents how many patterns each layer is looking for and what the depth of our response map will be. If we are looking for 32 different patterns/filters than our output feature map (aka the response map) will have a depth of 32. Each one of the 32 layers of depth will be a matrix of some size containing values indicating if the filter was present at that location or not.\n",
        "\n",
        "Here's a great illustration from the book \"Deep Learning with Python\" by Francois Chollet (pg 124).\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1HcLvvLKvLCCGuGZPMvKYz437FbbCC2eB)\n",
        "\n",
        "####**Sample Size**\n",
        "This isn't really the best term to describe this, but each convolutional layer is going to examine n x m blocks of pixels in each image. Typically, we'll consider 3x3 or 5x5 blocks. In the example above we use a 3x3 \"sample size\". This size will be the same as the size of our filter.\n",
        "\n",
        "Our layers work by sliding these filters of n x m pixels over every possible position in our image and populating a new feature map/response map indicating whether the filter is present at each location.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnzqr8Dzjchd"
      },
      "source": [
        "##Borders and Padding\n",
        "The more mathematical of you may have realized that if we slide a filter of let's say size 3x3 over our image well consider less positions for our filter than pixels in our input. Look at the example below.\n",
        "\n",
        "*Image from \"Deep Learning with Python\" by Francois Chollet (pg 126).*\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1OEfXrV16NBjwAafgBfYYcWOyBCHqaZ5M)\n",
        "\n",
        "This means our response map will have a slightly smaller width and height than our original image. This is fine but sometimes we want our response map to have the same dimensions. We can accomplish this by using something called *padding*.\n",
        "\n",
        "**Padding** is simply the addition of the appropriate number of rows and/or columns to your input data such that each pixel can be centered by the filter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDwH2eOMmt_N"
      },
      "source": [
        "##Strides\n",
        "In the previous sections we assumed that the filters would be slid continously through the image such that it covered every possible position. This is common but sometimes we introduce the idea of a **stride** to our convolutional layer. The stride size reprsents how many rows/cols we will move the filter each time. These are not used very frequently so we'll move on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCsVC-4UnfC8"
      },
      "source": [
        "##Pooling\n",
        "You may recall that our convnets are made up of a stack of convolution and pooling layers.\n",
        "\n",
        "The idea behind a pooling layer is to downsample our feature maps and reduce their dimensions. They work in a similar way to convolutional layers where they extract windows from the feature map and return a response map of the max, min or average values of each channel. Pooling is usually done using windows of size 2x2 and a stride of 2. This will reduce the size of the feature map by a factor of two and return a response map that is 2x smaller."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qo85O0LsxbB"
      },
      "source": [
        "##A More Detailed Look\n",
        "Please refer to the video to learn how all of this happens at the lower level!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqLsm2XzNQSE"
      },
      "source": [
        "##Creating a Convnet\n",
        "\n",
        "Now it is time to create our first convnet! This example is for the purpose of getting familiar with CNN architectures, we will talk about how to improves its performance later.\n",
        "\n",
        "*This tutorial is based on the following guide from the TensorFlow documentation: https://www.tensorflow.org/tutorials/images/cnn*\n",
        "\n",
        "###Dataset\n",
        "The problem we will consider here is classifying 10 different everyday objects. The dataset we will use is built into tensorflow and called the [**CIFAR Image Dataset.**](https://www.cs.toronto.edu/~kriz/cifar.html) It contains 60,000 32x32 color images with 6000 images of each class.\n",
        "\n",
        "The labels in this dataset are the following:\n",
        "- Airplane\n",
        "- Automobile\n",
        "- Bird\n",
        "- Cat\n",
        "- Deer\n",
        "- Dog\n",
        "- Frog\n",
        "- Horse\n",
        "- Ship\n",
        "- Truck\n",
        "\n",
        "We'll load the dataset and have a look at some of the images below.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnIbwiK7Ohv2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba8f673d-5359-44be-c52d-45b20990d202"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49wbEaM1PCCR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76259eba-b6a9-4527-8f4b-4abd203d8651"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "#  LOAD AND SPLIT DATASET\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "\n",
        "class_names = [ 'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bp0yAAcuPHFN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "9da73a9b-9784-40d0-9693-4053fc9573a0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Let's look at a one image\n",
        "IMG_INDEX = 7  # change this to look at other images\n",
        "\n",
        "plt.imshow(train_images[IMG_INDEX] ,cmap=plt.cm.binary)\n",
        "plt.xlabel(class_names[train_labels[IMG_INDEX][0]])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPqeddhcPwpc"
      },
      "source": [
        "##CNN Architecture\n",
        "A common architecture for a CNN is a stack of Conv2D and MaxPooling2D layers followed by a few denesly connected layers. To idea is that the stack of convolutional and maxPooling layers extract the features from the image. Then these features are flattened and fed to densly connected layers that determine the class of an image based on the presence of features.\n",
        "\n",
        "We will start by building the **Convolutional Base**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibuJZqAXQrWJ"
      },
      "outputs": [],
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tybTBoi_Qtxl"
      },
      "source": [
        "**Layer 1**\n",
        "\n",
        "The input shape of our data will be 32, 32, 3 and we will process 32 filters of size 3x3 over our input data. We will also apply the activation function relu to the output of each convolution operation.\n",
        "\n",
        "**Layer 2**\n",
        "\n",
        "This layer will perform the max pooling operation using 2x2 samples and a stride of 2.\n",
        "\n",
        "**Other Layers**\n",
        "\n",
        "The next set of layers do very similar things but take as input the feature map from the previous layer. They also increase the frequency of filters from 32 to 64. We can do this as our data shrinks in spacial dimensions as it passed through the layers, meaning we can afford (computationally) to add more depth."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QahwuduSEDG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "bbb6db27-998e-4655-e65b-dfe6d287e689"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m896\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m18,496\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │          \u001b[38;5;34m36,928\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m56,320\u001b[0m (220.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">56,320</span> (220.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m56,320\u001b[0m (220.00 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">56,320</span> (220.00 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model.summary()  # let's have a look at our model so far"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXw-sreaSzTW"
      },
      "source": [
        "After looking at the summary you should notice that the depth of our image increases but the spacial dimensions reduce drastically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjtADcfmSI9q"
      },
      "source": [
        "##Adding Dense Layers\n",
        "So far, we have just completed the **convolutional base**. Now we need to take these extracted features and add a way to classify them. This is why we add the following layers to our model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9TMZH_oSULo"
      },
      "outputs": [],
      "source": [
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEzHX-7ESeCl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "3fea600c-2155-45f2-a0c3-e7fee908be16"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m896\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m18,496\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │          \u001b[38;5;34m36,928\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m65,600\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │             \u001b[38;5;34m650\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">65,600</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m122,570\u001b[0m (478.79 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">122,570</span> (478.79 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m122,570\u001b[0m (478.79 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">122,570</span> (478.79 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxfqtdDbSf4W"
      },
      "source": [
        "We can see that the flatten layer changes the shape of our data so that we can feed it to the 64-node dense layer, follwed by the final output layer of 10 neurons (one for each class).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdPxFvHdTLRK"
      },
      "source": [
        "##Training\n",
        "Now we will train and compile the model using the recommended hyper paramaters from tensorflow.\n",
        "\n",
        "*Note: This will take much longer than previous models!*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5loIug93TW1E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a34f581-d6db-4fad-d8e9-b294b2b02f86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 27ms/step - accuracy: 0.3538 - loss: 1.7510 - val_accuracy: 0.5317 - val_loss: 1.3099\n",
            "Epoch 2/4\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 26ms/step - accuracy: 0.5716 - loss: 1.2071 - val_accuracy: 0.6177 - val_loss: 1.0823\n",
            "Epoch 3/4\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 26ms/step - accuracy: 0.6431 - loss: 1.0282 - val_accuracy: 0.6675 - val_loss: 0.9669\n",
            "Epoch 4/4\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 26ms/step - accuracy: 0.6765 - loss: 0.9238 - val_accuracy: 0.6669 - val_loss: 0.9490\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_images, train_labels, epochs=4,\n",
        "                    validation_data=(test_images, test_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkdRKQnETgLv"
      },
      "source": [
        "##Evaluating the Model\n",
        "We can determine how well the model performed by looking at it's performance on the test data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6I2vJFiiTkQE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c7a12be-3912-48ad-a919-329bf6850924"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 2s - 8ms/step - accuracy: 0.6669 - loss: 0.9490\n",
            "0.6668999791145325\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "print(test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lKwDlvvUbIm"
      },
      "source": [
        "You should be getting an accuracy of about 70%. This isn't bad for a simple model like this, but we'll dive into some better approaches for computer vision below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cstpZFVaY7YH"
      },
      "source": [
        "##Working with Small Datasets\n",
        "In the situation where you don't have millions of images it is difficult to train a CNN from scratch that performs very well. This is why we will learn about a few techniques we can use to train CNN's on small datasets of just a few thousand images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D4iWJ17ZRt_"
      },
      "source": [
        "###Data Augmentation\n",
        "To avoid overfitting and create a larger dataset from a smaller one we can use a technique called data augmentation. This is simply performing random transofrmations on our images so that our model can generalize better. These transformations can be things like compressions, rotations, stretches and even color changes.\n",
        "\n",
        "Fortunately, keras can help us do this. Look at the code below to an example of data augmentation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sOet0hQZ-gR"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# creates a data generator object that transforms images\n",
        "datagen = ImageDataGenerator(\n",
        "rotation_range=40,\n",
        "width_shift_range=0.2,\n",
        "height_shift_range=0.2,\n",
        "shear_range=0.2,\n",
        "zoom_range=0.2,\n",
        "horizontal_flip=True,\n",
        "fill_mode='nearest')\n",
        "\n",
        "# pick an image to transform\n",
        "test_img = train_images[20]\n",
        "img = image.img_to_array(test_img)  # convert image to numpy arry\n",
        "img = img.reshape((1,) + img.shape)  # reshape image\n",
        "\n",
        "i = 0\n",
        "\n",
        "for batch in datagen.flow(img, save_prefix='test', save_format='jpeg'):  # this loops runs forever until we break, saving images to current directory with specified prefix\n",
        "    plt.figure(i)\n",
        "    plot = plt.imshow(image.img_to_array(batch[0]))\n",
        "    i += 1\n",
        "    if i > 4:  # show 4 images\n",
        "        break\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nc9RyHPYUnSK"
      },
      "source": [
        "###Pretrained Models\n",
        "You would have noticed that the model above takes a few minutes to train in the NoteBook and only gives an accuaracy of ~70%. This is okay but surely there is a way to improve on this.\n",
        "\n",
        "In this section we will talk about using a pretrained CNN as apart of our own custom network to improve the accuracy of our model. We know that CNN's alone (with no dense layers) don't do anything other than map the presence of features from our input. This means we can use a pretrained CNN, one trained on millions of images, as the start of our model. This will allow us to have a very good convolutional base before adding our own dense layered classifier at the end. In fact, by using this techique we can train a very good classifier for a realtively small dataset (< 10,000 images). This is because the convnet already has a very good idea of what features to look for in an image and can find them very effectively. So, if we can determine the presence of features all the rest of the model needs to do is determine which combination of features makes a specific image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u10oZO1oXT6Y"
      },
      "source": [
        "###Fine Tuning\n",
        "When we employ the technique defined above, we will often want to tweak the final layers in our convolutional base to work better for our specific problem. This involves not touching or retraining the earlier layers in our convolutional base but only adjusting the final few. We do this because the first layers in our base are very good at extracting low level features lile lines and edges, things that are similar for any kind of image. Where the later layers are better at picking up very specific features like shapes or even eyes. If we adjust the final layers than we can look for only features relevant to our very specific problem.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XolyariNdj5p"
      },
      "source": [
        "##Using a Pretrained Model\n",
        "In this section we will combine the tecniques we learned above and use a pretrained model and fine tuning to classify images of dogs and cats using a small dataset.\n",
        "\n",
        "*This tutorial is based on the following guide from the TensorFlow documentation: https://www.tensorflow.org/tutorials/images/transfer_learning*\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nRe9qWmgxm7"
      },
      "outputs": [],
      "source": [
        "#Imports\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "keras = tf.keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUx4I_4jg2Tc"
      },
      "source": [
        "###Dataset\n",
        "We will load the *cats_vs_dogs* dataset from the modoule tensorflow_datatsets.\n",
        "\n",
        "This dataset contains (image, label) pairs where images have different dimensions and 3 color channels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuGu50NlgreO",
        "outputId": "faf30900-684e-42bb-d838-e085c20edf6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset 786.67 MiB (download: 786.67 MiB, generated: 1.04 GiB, total: 1.81 GiB) to /root/tensorflow_datasets/cats_vs_dogs/4.0.1...\n"
          ]
        }
      ],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "# split the data manually into 80% training, 10% testing, 10% validation\n",
        "(raw_train, raw_validation, raw_test), metadata = tfds.load(\n",
        "    'cats_vs_dogs',\n",
        "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mk_MpiQyh-as"
      },
      "outputs": [],
      "source": [
        "get_label_name = metadata.features['label'].int2str  # creates a function object that we can use to get labels\n",
        "# display 2 images from the dataset\n",
        "for image, label in raw_train.take(5):\n",
        "  plt.figure()\n",
        "  plt.imshow(image)\n",
        "  plt.title(get_label_name(label))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCdodmcYiPOF"
      },
      "source": [
        "###Data Preprocessing\n",
        "Since the sizes of our images are all different, we need to convert them all to the same size. We can create a function that will do that for us below.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcoKn1VUieqx"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 160 # All images will be resized to 160x160\n",
        "\n",
        "def format_example(image, label):\n",
        "  \"\"\"\n",
        "  returns an image that is reshaped to IMG_SIZE\n",
        "  \"\"\"\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image = (image/127.5) - 1\n",
        "  image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "  return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwIB21lailXh"
      },
      "source": [
        "Now we can apply this function to all our images using ```.map()```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0E8iqYOAipdU"
      },
      "outputs": [],
      "source": [
        "train = raw_train.map(format_example)\n",
        "validation = raw_validation.map(format_example)\n",
        "test = raw_test.map(format_example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QORLTVNaiqym"
      },
      "source": [
        "Let's have a look at our images now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dU5JIa2Jiv9U"
      },
      "outputs": [],
      "source": [
        "for image, label in train.take(2):\n",
        "  plt.figure()\n",
        "  plt.imshow(image)\n",
        "  plt.title(get_label_name(label))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFnFVaNQi7Vq"
      },
      "source": [
        "Finally we will shuffle and batch the images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5ZIhkFPi_Pb"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "SHUFFLE_BUFFER_SIZE = 1000\n",
        "train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "validation_batches = validation.batch(BATCH_SIZE)\n",
        "test_batches = test.batch(BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QxI-fOAjDzC"
      },
      "source": [
        "Now if we look at the shape of an original image vs the new image we will see it has been changed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyqrCYNOjY9v"
      },
      "outputs": [],
      "source": [
        "for img, label in raw_train.take(2):\n",
        "  print(\"Original shape:\", img.shape)\n",
        "\n",
        "for img, label in train.take(2):\n",
        "  print(\"New shape:\", img.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMpKJ3Xbj4BW"
      },
      "source": [
        "###Picking a Pretrained Model\n",
        "The model we are going to use as the convolutional base for our model is the **MobileNet V2** developed at Google. This model is trained on 1.4 million images and has 1000 different classes.\n",
        "\n",
        "We want to use this model but only its convolutional base. So, when we load in the model, we'll specify that we don't want to load the top (classification) layer. We'll tell the model what input shape to expect and to use the predetermined weights from *imagenet* (Googles dataset).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2a09os6dkokI"
      },
      "outputs": [],
      "source": [
        "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
        "\n",
        "# Create the base model from the pre-trained model MobileNet V2\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
        "                                               include_top= False,\n",
        "                                               weights='imagenet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uRvMuWoFR2CO"
      },
      "outputs": [],
      "source": [
        "base_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckYqfl7Vky3S"
      },
      "source": [
        "At this point this *base_model* will simply output a shape (32, 5, 5, 1280) tensor that is a feature extraction from our original (1, 160, 160, 3) image. The 32 means that we have 32 layers of differnt filters/features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yojo6ONzlFGF"
      },
      "outputs": [],
      "source": [
        "for image, _ in train_batches.take(1):\n",
        "   pass\n",
        "\n",
        "feature_batch = base_model(image)\n",
        "print(feature_batch.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ2kn1P_lhsg"
      },
      "source": [
        "###Freezing the Base\n",
        "The term **freezing** refers to disabling the training property of a layer. It simply means we won’t make any changes to the weights of any layers that are frozen during training. This is important as we don't want to change the convolutional base that already has learned weights.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hXctqtYl8o5"
      },
      "outputs": [],
      "source": [
        "base_model.trainable = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jIGFXOrl9wc"
      },
      "outputs": [],
      "source": [
        "base_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7UJLbJ7mJzw"
      },
      "source": [
        "###Adding our Classifier\n",
        "Now that we have our base layer setup, we can add the classifier. Instead of flattening the feature map of the base layer we will use a global average pooling layer that will average the entire 5x5 area of each 2D feature map and return to us a single 1280 element vector per filter.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uUwG5wrnFD6"
      },
      "outputs": [],
      "source": [
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejxd7rjInIRp"
      },
      "source": [
        "Finally, we will add the predicition layer that will be a single dense neuron. We can do this because we only have two classes to predict for.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GA-iVZj9nH_N"
      },
      "outputs": [],
      "source": [
        "prediction_layer = keras.layers.Dense(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn9G9KiFnXu6"
      },
      "source": [
        "Now we will combine these layers together in a model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_IJucQNnXBK"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "  base_model,\n",
        "  global_average_layer,\n",
        "  prediction_layer\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLYdAL2uSt_a"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHepCsPXnpYZ"
      },
      "source": [
        "###Training the Model\n",
        "Now we will train and compile the model. We will use a very small learning rate to ensure that the model does not have any major changes made to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQhg2WxHnxra"
      },
      "outputs": [],
      "source": [
        "base_learning_rate = 0.0001\n",
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Fx9nySdoZuL"
      },
      "outputs": [],
      "source": [
        "# We can evaluate the model right now to see how it does before training it on our new images\n",
        "initial_epochs = 3\n",
        "validation_steps=20\n",
        "\n",
        "loss0,accuracy0 = model.evaluate(validation_batches, steps = validation_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edMXObctojl6"
      },
      "outputs": [],
      "source": [
        "# Now we can train it on our images\n",
        "history = model.fit(train_batches,\n",
        "                    epochs=initial_epochs,\n",
        "                    validation_data=validation_batches)\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "print(acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUUt3AxA2lf2"
      },
      "outputs": [],
      "source": [
        "model.save(\"dogs_vs_cats.h5\")  # we can save the model and reload it at anytime in the future\n",
        "new_model = tf.keras.models.load_model('dogs_vs_cats.h5')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MVJVsNfNp6LX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Applying GradientTape for customized loss function.\n",
        "# Define model and loss\n",
        "model = tf.keras.Sequential([tf.keras.layers.Dense(1)])\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "# Use Adam optimizer (which has an adaptive learning rate)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
        "\n",
        "# Training step\n",
        "with tf.GradientTape() as tape:\n",
        "    predictions = model(tf.constant([[1.0], [2.0], [3.0]]))\n",
        "    loss = loss_fn(tf.constant([[2.0], [4.0], [6.0]]), predictions)\n",
        "# Compute and apply gradients\n",
        "grads = tape.gradient(loss, model.trainable_variables)\n",
        "optimizer.apply_gradients(zip(grads, model.trainable_variables))"
      ],
      "metadata": {
        "id": "0j9qgXRdp8cD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using GPU for training"
      ],
      "metadata": {
        "id": "0OL-H3w2qQ1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available:\", len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvimeL1CrBor",
        "outputId": "f0db05f7-f9aa-411d-d03f-44fa7ef60695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.config.list_physical_devices('GPU'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMGU523XrNNQ",
        "outputId": "e79a7c09-f967-4567-9e58-3a2002922741"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VlyZMz6IsK63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Load dataset\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Define model\n",
        "model = keras.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28)),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model (TensorFlow will automatically use the GPU)\n",
        "with tf.device('/GPU:0'):\n",
        "  history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    validation_data=(x_test, y_test))\n",
        "\n",
        "print(acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjMwd6G3sllC",
        "outputId": "1c46cac8-e8a9-4df5-e760-b80cb377b4a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8749 - loss: 0.4412 - val_accuracy: 0.9583 - val_loss: 0.1391\n",
            "Epoch 2/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9645 - loss: 0.1249 - val_accuracy: 0.9723 - val_loss: 0.0959\n",
            "Epoch 3/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9755 - loss: 0.0803 - val_accuracy: 0.9732 - val_loss: 0.0832\n",
            "Epoch 4/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9825 - loss: 0.0578 - val_accuracy: 0.9719 - val_loss: 0.0873\n",
            "Epoch 5/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9867 - loss: 0.0442 - val_accuracy: 0.9743 - val_loss: 0.0837\n",
            "Epoch 6/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3ms/step - accuracy: 0.9894 - loss: 0.0343 - val_accuracy: 0.9786 - val_loss: 0.0759\n",
            "Epoch 7/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9919 - loss: 0.0261 - val_accuracy: 0.9772 - val_loss: 0.0736\n",
            "Epoch 8/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9943 - loss: 0.0202 - val_accuracy: 0.9739 - val_loss: 0.0934\n",
            "Epoch 9/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9944 - loss: 0.0186 - val_accuracy: 0.9779 - val_loss: 0.0797\n",
            "Epoch 10/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.9954 - loss: 0.0156 - val_accuracy: 0.9768 - val_loss: 0.0831\n",
            "[0.9251833558082581, 0.966533362865448, 0.9766499996185303, 0.9827499985694885, 0.9860666394233704, 0.9890000224113464, 0.9909833073616028, 0.9926499724388123, 0.9941999912261963, 0.9952499866485596]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8YcdmWUvYae"
      },
      "source": [
        "##Object Detection\n",
        "If you'd like to learn how you can perform object detection and recognition with tensorflow check out the guide below.\n",
        "\n",
        "https://github.com/tensorflow/models/tree/master/research/object_detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEiX-D2f2tvI"
      },
      "source": [
        "##Sources\n",
        "1. “Convolutional Neural Network (CNN) &nbsp;: &nbsp; TensorFlow Core.” TensorFlow, www.tensorflow.org/tutorials/images/cnn.\n",
        "2. “Transfer Learning with a Pretrained ConvNet &nbsp;: &nbsp; TensorFlow Core.” TensorFlow, www.tensorflow.org/tutorials/images/transfer_learning.\n",
        "3. Chollet François. Deep Learning with Python. Manning Publications Co., 2018.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}